2019-01-13 15:36:58,863 ****************************************************************************************************
2019-01-13 15:36:59,350 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:36:59,351 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:36:59,398 X_train features: (401, 68466);
2019-01-13 15:36:59,398 sympt_train_labels: (401, 92)
2019-01-13 15:36:59,398 X_test features: (45, 68466);
2019-01-13 15:36:59,398 sympt_test_labels: (45, 92)
2019-01-13 15:36:59,399 {'estimator__C': 1.2755478561043065, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=1.2755478561043065, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:37:29,642 ****************************************************************************************************
2019-01-13 15:37:30,161 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:37:30,161 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:37:30,207 X_train features: (401, 68466);
2019-01-13 15:37:30,207 sympt_train_labels: (401, 92)
2019-01-13 15:37:30,208 X_test features: (45, 68466);
2019-01-13 15:37:30,208 sympt_test_labels: (45, 92)
2019-01-13 15:37:30,208 {'estimator__C': 8.169596147501819, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=8.169596147501819, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:37:51,946 ****************************************************************************************************
2019-01-13 15:37:52,435 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:37:52,435 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:37:52,484 X_train features: (401, 68466);
2019-01-13 15:37:52,484 sympt_train_labels: (401, 92)
2019-01-13 15:37:52,484 X_test features: (45, 68466);
2019-01-13 15:37:52,484 sympt_test_labels: (45, 92)
2019-01-13 15:37:52,484 {'estimator__C': 6.4804353943131625, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=6.4804353943131625, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:38:33,849 ****************************************************************************************************
2019-01-13 15:38:34,348 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:38:34,349 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:38:34,394 X_train features: (401, 68466);
2019-01-13 15:38:34,395 sympt_train_labels: (401, 92)
2019-01-13 15:38:34,395 X_test features: (45, 68466);
2019-01-13 15:38:34,395 sympt_test_labels: (45, 92)
2019-01-13 15:38:34,395 {'estimator__C': 1.7010884939534647, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=1.7010884939534647, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:38:51,747 ****************************************************************************************************
2019-01-13 15:38:52,223 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:38:52,224 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:38:52,270 X_train features: (401, 68466);
2019-01-13 15:38:52,271 sympt_train_labels: (401, 92)
2019-01-13 15:38:52,271 X_test features: (45, 68466);
2019-01-13 15:38:52,271 sympt_test_labels: (45, 92)
2019-01-13 15:38:52,271 {'estimator__C': 7.789161228100912, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=7.789161228100912, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:39:20,467 ****************************************************************************************************
2019-01-13 15:39:20,911 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:39:20,912 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:39:20,957 X_train features: (401, 68466);
2019-01-13 15:39:20,957 sympt_train_labels: (401, 92)
2019-01-13 15:39:20,957 X_test features: (45, 68466);
2019-01-13 15:39:20,958 sympt_test_labels: (45, 92)
2019-01-13 15:39:20,958 {'estimator__C': 1.4690896684628703, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=1.4690896684628703, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:39:42,512 ****************************************************************************************************
2019-01-13 15:39:42,993 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:39:42,994 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:39:43,040 X_train features: (402, 68466);
2019-01-13 15:39:43,040 sympt_train_labels: (402, 92)
2019-01-13 15:39:43,040 X_test features: (44, 68466);
2019-01-13 15:39:43,041 sympt_test_labels: (44, 92)
2019-01-13 15:39:43,041 {'estimator__C': 3.1735796924204944, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=3.1735796924204944, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:40:03,651 ****************************************************************************************************
2019-01-13 15:40:04,156 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:40:04,157 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:40:04,202 X_train features: (402, 68466);
2019-01-13 15:40:04,203 sympt_train_labels: (402, 92)
2019-01-13 15:40:04,203 X_test features: (44, 68466);
2019-01-13 15:40:04,203 sympt_test_labels: (44, 92)
2019-01-13 15:40:04,203 {'estimator__C': 6.4804353943131625, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=6.4804353943131625, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:40:46,079 ****************************************************************************************************
2019-01-13 15:40:46,621 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:40:46,622 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:40:46,669 X_train features: (402, 68466);
2019-01-13 15:40:46,669 sympt_train_labels: (402, 92)
2019-01-13 15:40:46,669 X_test features: (44, 68466);
2019-01-13 15:40:46,669 sympt_test_labels: (44, 92)
2019-01-13 15:40:46,670 {'estimator__C': 5.994644084225208, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=5.994644084225208, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:41:07,714 ****************************************************************************************************
2019-01-13 15:41:08,229 {'input': 'content', 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'encoding': 'utf-8', 'dtype': <class 'numpy.int64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': {'но', 'зачем', 'эту', 'теперь', 'уж', 'том', 'из', 'вы', 'без', 'можно', 'нас', 'ним', 'я', 'над', 'между', 'чтоб', 'моя', 'там', 'один', 'нет', 'если', 'тебя', 'какой', 'опять', 'при', 'наконец', 'всю', 'себе', 'них', 'об', 'сам', 'с', 'ней', 'совсем', 'или', 'никогда', 'впрочем', 'его', 'и', 'до', 'он', 'надо', 'была', 'нельзя', 'раз', 'а', 'этом', 'два', 'чуть', 'их', 'будто', 'к', 'со', 'меня', 'него', 'для', 'есть', 'мы', 'хоть', 'чтобы', 'этой', 'такой', 'иногда', 'бы', 'на', 'было', 'будет', 'были', 'разве', 'ни', 'всегда', 'про', 'только', 'после', 'того', 'здесь', 'себя', 'конечно', 'тогда', 'три', 'мне', 'она', 'почти', 'какая', 'что', 'еще', 'через', 'тоже', 'больше', 'ж', 'может', 'о', 'вам', 'ведь', 'эти', 'этого', 'другой', 'не', 'быть', 'все', 'ее', 'куда', 'же', 'был', 'перед', 'этот', 'тут', 'от', 'вас', 'по', 'как', 'они', 'нибудь', 'когда', 'даже', 'потому', 'ли', 'в', 'тем', 'ему', 'вдруг', 'лучше', 'ничего', 'свою', 'у', 'сейчас', 'ей', 'под', 'мой', 'ну', 'то', 'много', 'да', 'так', 'во', 'кто', 'хорошо', 'чем', 'потом', 'за', 'им', 'нее', 'всего', 'ты', 'всех', 'уже', 'где', 'тот', 'чего', 'более', 'вот'}, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:41:08,230 {'use_idf': True, 'norm': 'l2', 'input': 'content', 'sublinear_tf': False, 'max_features': None, 'max_df': 1.0, 'binary': False, 'analyzer': 'word', 'preprocessor': None, 'smooth_idf': True, 'encoding': 'utf-8', 'dtype': <class 'numpy.float64'>, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'min_df': 1, 'lowercase': True, 'stop_words': None, 'decode_error': 'strict', 'tokenizer': None, 'vocabulary': None, 'strip_accents': None, 'ngram_range': (1, 5)}
2019-01-13 15:41:08,282 X_train features: (402, 68466);
2019-01-13 15:41:08,283 sympt_train_labels: (402, 92)
2019-01-13 15:41:08,283 X_test features: (44, 68466);
2019-01-13 15:41:08,283 sympt_test_labels: (44, 92)
2019-01-13 15:41:08,283 {'estimator__C': 7.5166669815795, 'estimator__tol': 0.0001, 'estimator__multi_class': 'ovr', 'estimator__intercept_scaling': 1, 'estimator__n_jobs': None, 'n_jobs': -1, 'estimator__class_weight': None, 'estimator__max_iter': 1000, 'estimator__warm_start': False, 'estimator__solver': 'warn', 'estimator': LogisticRegression(C=7.5166669815795, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=1000,
          multi_class='ovr', n_jobs=None, penalty='l1', random_state=1024,
          solver='warn', tol=0.0001, verbose=0, warm_start=False), 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__verbose': 0, 'estimator__dual': False, 'estimator__fit_intercept': True}
2019-01-13 15:41:29,768 +----+----------+-------------+----------+----------+-----------+
|    |   fold_# |   precision |   recall |       f1 |   jaccard |
|----+----------+-------------+----------+----------+-----------|
|  0 |        1 |    0.622899 | 0.504348 | 0.53604  |  0.443704 |
|  1 |        2 |    0.634085 | 0.535088 | 0.550344 |  0.466111 |
|  2 |        3 |    0.755195 | 0.718182 | 0.724412 |  0.66963  |
|  3 |        4 |    0.595455 | 0.463636 | 0.504004 |  0.396481 |
|  4 |        5 |    0.683471 | 0.570248 | 0.592731 |  0.54537  |
|  5 |        6 |    0.676522 | 0.591304 | 0.607708 |  0.537831 |
|  6 |        7 |    0.607975 | 0.508065 | 0.529215 |  0.453274 |
|  7 |        8 |    0.680435 | 0.582609 | 0.602029 |  0.524892 |
|  8 |        9 |    0.6403   | 0.579365 | 0.592753 |  0.56553  |
|  9 |       10 |    0.727135 | 0.644628 | 0.667674 |  0.646212 |
+----+----------+-------------+----------+----------+-----------+
2019-01-13 15:41:29,769 CV score f1 : Mean - 0.5906910 | Std - 0.0667856 | Min - 0.5040043 | Max - 0.7244123
2019-01-13 15:41:29,770 CV score jaccard : Mean - 0.5249035 | Std - 0.0879210 | Min - 0.3964815 | Max - 0.6696296


