2019-01-12 12:04:32,418 ****************************************************************************************************
2019-01-12 12:04:32,655 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 12:04:32,655 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 12:04:32,678 X_train features: (401, 68466);
2019-01-12 12:04:32,678 sympt_train_labels: (401, 92)
2019-01-12 12:04:32,679 X_test features: (45, 68466);
2019-01-12 12:04:32,679 sympt_test_labels: (45, 92)
2019-01-12 12:04:32,679 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 12:15:55,637 best_params: {'target': 0.5360399021268587, 'params': {'C': 1.2755478561043065}}
2019-01-12 12:15:55,638 ****************************************************************************************************
2019-01-12 12:15:55,864 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 12:15:55,864 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 12:15:55,890 X_train features: (401, 68466);
2019-01-12 12:15:55,890 sympt_train_labels: (401, 92)
2019-01-12 12:15:55,890 X_test features: (45, 68466);
2019-01-12 12:15:55,890 sympt_test_labels: (45, 92)
2019-01-12 12:15:55,891 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 12:32:31,237 best_params: {'target': 0.5503438082385451, 'params': {'C': 8.169596147501819}}
2019-01-12 12:32:31,237 ****************************************************************************************************
2019-01-12 12:32:31,462 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 12:32:31,462 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 12:32:31,484 X_train features: (401, 68466);
2019-01-12 12:32:31,484 sympt_train_labels: (401, 92)
2019-01-12 12:32:31,485 X_test features: (45, 68466);
2019-01-12 12:32:31,485 sympt_test_labels: (45, 92)
2019-01-12 12:32:31,485 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 13:12:56,301 best_params: {'target': 0.7244122544122544, 'params': {'C': 6.4804353943131625}}
2019-01-12 13:12:56,301 ****************************************************************************************************
2019-01-12 13:12:56,515 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 13:12:56,516 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 13:12:56,537 X_train features: (401, 68466);
2019-01-12 13:12:56,538 sympt_train_labels: (401, 92)
2019-01-12 13:12:56,538 X_test features: (45, 68466);
2019-01-12 13:12:56,538 sympt_test_labels: (45, 92)
2019-01-12 13:12:56,538 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 13:28:02,433 best_params: {'target': 0.504004329004329, 'params': {'C': 1.7010884939534647}}
2019-01-12 13:28:02,433 ****************************************************************************************************
2019-01-12 13:28:02,653 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 13:28:02,654 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 13:28:02,676 X_train features: (401, 68466);
2019-01-12 13:28:02,676 sympt_train_labels: (401, 92)
2019-01-12 13:28:02,677 X_test features: (45, 68466);
2019-01-12 13:28:02,677 sympt_test_labels: (45, 92)
2019-01-12 13:28:02,677 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 13:43:30,117 best_params: {'target': 0.5927313274420711, 'params': {'C': 7.789161228100912}}
2019-01-12 13:43:30,117 ****************************************************************************************************
2019-01-12 13:43:30,333 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 13:43:30,334 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 13:43:30,356 X_train features: (401, 68466);
2019-01-12 13:43:30,356 sympt_train_labels: (401, 92)
2019-01-12 13:43:30,356 X_test features: (45, 68466);
2019-01-12 13:43:30,356 sympt_test_labels: (45, 92)
2019-01-12 13:43:30,356 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 13:56:39,437 best_params: {'target': 0.6077075098814229, 'params': {'C': 1.4690896684628703}}
2019-01-12 13:56:39,438 ****************************************************************************************************
2019-01-12 13:56:39,655 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 13:56:39,655 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 13:56:39,677 X_train features: (402, 68466);
2019-01-12 13:56:39,678 sympt_train_labels: (402, 92)
2019-01-12 13:56:39,678 X_test features: (44, 68466);
2019-01-12 13:56:39,678 sympt_test_labels: (44, 92)
2019-01-12 13:56:39,678 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 14:07:24,477 best_params: {'target': 0.5292146697388632, 'params': {'C': 3.1735796924204944}}
2019-01-12 14:07:24,477 ****************************************************************************************************
2019-01-12 14:07:24,701 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 14:07:24,701 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 14:07:24,723 X_train features: (402, 68466);
2019-01-12 14:07:24,723 sympt_train_labels: (402, 92)
2019-01-12 14:07:24,723 X_test features: (44, 68466);
2019-01-12 14:07:24,723 sympt_test_labels: (44, 92)
2019-01-12 14:07:24,724 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 14:23:58,418 best_params: {'target': 0.6020289855072464, 'params': {'C': 6.4804353943131625}}
2019-01-12 14:23:58,418 ****************************************************************************************************
2019-01-12 14:23:58,631 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 14:23:58,631 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 14:23:58,653 X_train features: (402, 68466);
2019-01-12 14:23:58,653 sympt_train_labels: (402, 92)
2019-01-12 14:23:58,653 X_test features: (44, 68466);
2019-01-12 14:23:58,653 sympt_test_labels: (44, 92)
2019-01-12 14:23:58,654 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 14:40:13,744 best_params: {'target': 0.5927532123960696, 'params': {'C': 5.994644084225208}}
2019-01-12 14:40:13,744 ****************************************************************************************************
2019-01-12 14:40:13,957 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': {'от', 'другой', 'нее', 'более', 'я', 'без', 'чуть', 'ней', 'этой', 'него', 'здесь', 'на', 'эту', 'вам', 'ему', 'как', 'об', 'ну', 'этого', 'почти', 'с', 'ним', 'если', 'а', 'вдруг', 'нас', 'к', 'том', 'какая', 'был', 'тот', 'сейчас', 'тогда', 'она', 'один', 'про', 'этом', 'всю', 'тут', 'не', 'ли', 'больше', 'два', 'будто', 'для', 'можно', 'же', 'уж', 'конечно', 'зачем', 'за', 'они', 'вы', 'и', 'было', 'эти', 'даже', 'надо', 'раз', 'моя', 'нельзя', 'о', 'ни', 'по', 'во', 'тебя', 'меня', 'у', 'была', 'всегда', 'там', 'свою', 'много', 'перед', 'ведь', 'ее', 'чего', 'где', 'ты', 'только', 'куда', 'кто', 'ей', 'но', 'мне', 'или', 'может', 'тем', 'иногда', 'были', 'наконец', 'вас', 'хоть', 'им', 'себе', 'через', 'да', 'есть', 'того', 'после', 'мы', 'еще', 'впрочем', 'быть', 'чтоб', 'то', 'хорошо', 'никогда', 'нибудь', 'их', 'всего', 'в', 'три', 'чем', 'над', 'ничего', 'опять', 'со', 'из', 'разве', 'будет', 'все', 'так', 'между', 'что', 'какой', 'лучше', 'его', 'сам', 'себя', 'тоже', 'он', 'такой', 'потом', 'при', 'бы', 'мой', 'уже', 'вот', 'потому', 'до', 'теперь', 'чтобы', 'этот', 'под', 'всех', 'когда', 'них', 'совсем', 'нет', 'ж'}, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}
2019-01-12 14:40:13,958 {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}
2019-01-12 14:40:13,980 X_train features: (402, 68466);
2019-01-12 14:40:13,980 sympt_train_labels: (402, 92)
2019-01-12 14:40:13,980 X_test features: (44, 68466);
2019-01-12 14:40:13,980 sympt_test_labels: (44, 92)
2019-01-12 14:40:13,980 {'estimator__C': 10.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__max_iter': 100, 'estimator__multi_class': 'ovr', 'estimator__n_jobs': 1, 'estimator__penalty': 'l1', 'estimator__random_state': 1024, 'estimator__solver': 'liblinear', 'estimator__tol': 0.0001, 'estimator__verbose': 1, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1024, solver='liblinear', tol=0.0001,
          verbose=1, warm_start=False), 'n_jobs': -1}
2019-01-12 14:52:22,312 best_params: {'target': 0.6676740059515178, 'params': {'C': 7.5166669815795}}
2019-01-12 14:52:22,312 []
